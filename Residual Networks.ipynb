{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we will implement a very deep neural network using Residual Networks. \n",
    "\n",
    "Our framework of choice is Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Let's start by loading the packages and dependencies we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from utils import *\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    conv_name_base = f'res{stage}{block}_branch'\n",
    "    bn_name_base = f'bn{stage}{block}_branch'\n",
    "    \n",
    "    # Retrieve filters\n",
    "    filter_1, filter_2, filter_3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of the main path.\n",
    "    X = Conv2D(\n",
    "        filters=filter_1,\n",
    "        kernel_size=(1, 1),\n",
    "        strides=(1, 1),\n",
    "        padding='valid', \n",
    "        name=f'{conv_name_base}2a',\n",
    "        kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=f'{bn_name_base}2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of the main path\n",
    "    X = Conv2D(\n",
    "        filters=filter_2, \n",
    "        kernel_size=(f, f), \n",
    "        strides=(1, 1), \n",
    "        padding='same', \n",
    "        name=f'{conv_name_base}2b', \n",
    "        kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=f'{bn_name_base}2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Third component of the main path\n",
    "    X = Conv2D(\n",
    "        filters=filter_3, \n",
    "        kernel_size=(1, 1), \n",
    "        strides=(1, 1),\n",
    "        padding='valid', \n",
    "        name=f'{conv_name_base}2c',\n",
    "        kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=f'{bn_name_base}2c')(X)\n",
    "    \n",
    "    # Final step: Add shorcut value to main path and activate with ReLU.\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: [0.19716819 0.         1.3561226  2.1713073  0.         1.3324987 ]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as s:\n",
    "    np.random.seed(1)\n",
    "    previous_activations = tf.placeholder('float', [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    activations = identity_block(previous_activations, f=2, filters=[2, 4, 6], stage=1, block='a')\n",
    "    \n",
    "    s.run(tf.global_variables_initializer())\n",
    "    \n",
    "    out = s.run([activations], feed_dict={previous_activations: X, K.learning_phase(): 0})\n",
    "    print(f'out: {out[0][1][1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    # Defining name basis\n",
    "    conv_name_base = f'res{stage}{block}_branch'\n",
    "    bn_name_base = f'bn{stage}{block}_branch'\n",
    "    \n",
    "    # Retrieve filters\n",
    "    filter_1, filter_2, filter_3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(\n",
    "        filters=filter_1,\n",
    "        kernel_size=(1, 1),\n",
    "        strides=(s, s),\n",
    "        padding='valid',\n",
    "        name=f'{conv_name_base}2a',\n",
    "        kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=f'{bn_name_base}2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path\n",
    "    X = Conv2D(\n",
    "        filters=filter_2,\n",
    "        kernel_size=(f, f), \n",
    "        strides=(1, 1), \n",
    "        name=f'{conv_name_base}2b',\n",
    "        padding='same', \n",
    "        kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=f'{bn_name_base}2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Third component of main path\n",
    "    X = Conv2D(\n",
    "        filters=filter_3,\n",
    "        kernel_size=(1, 1), \n",
    "        strides=(1, 1), \n",
    "        name=f'{conv_name_base}2c', \n",
    "        padding='valid', \n",
    "        kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=f'{bn_name_base}2c')(X)\n",
    "    \n",
    "    # Shortcut path\n",
    "    X_shortcut = Conv2D(\n",
    "        filters=filter_3,\n",
    "        kernel_size=(1, 1), \n",
    "        strides=(s, s), \n",
    "        name=f'{conv_name_base}I', \n",
    "        padding='valid', \n",
    "        kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=f'{bn_name_base}I')(X_shortcut)\n",
    "    \n",
    "    # Add both main and shortcut path and pass them through a ReLU activation.\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: [0.09018463 1.2348979  0.46822023 0.03671762 0.         0.65516603]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as s:\n",
    "    np.random.seed(1)\n",
    "    previous_activations = tf.placeholder('float', [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    activations = convolutional_block(previous_activations, f=2, filters=[2, 4, 6], stage=1, block='a')\n",
    "    \n",
    "    s.run(tf.global_variables_initializer())\n",
    "    \n",
    "    out = s.run([activations], feed_dict={previous_activations: X, K.learning_phase(): 0})\n",
    "    print(f'out: {out[0][1][1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
